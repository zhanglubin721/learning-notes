# **1) 发送路径（客户端 → NameServer 路由发现 → Broker 写入）**

## **1.1 时序（同步发送示例）**

```
Producer.start
 └─ MQClientInstance.start
     ├─ 定时：updateTopicRouteInfoFromNameServer(topic…)
     └─ 心跳：sendHeartbeatToAllBroker
Producer.send(msg)
 ├─ 校验/压缩/属性设置（Message#setKeys/Tags/Delay/Trans…）
 ├─ 查路由 TopicPublishInfo（本地缓存命中 or 拉取）
 │   └─ mQClientAPI.getTopicRouteInfoFromNameServer(topic)
 │       └─ NameServer 返回 TopicRouteData（QueueData + BrokerData）
 ├─ 选队列 selectOneMessageQueue
 │   └─ 轮询 + 故障延迟规避（MQFaultStrategy / LatencyFaultTolerance）
 ├─ 发送 sendKernelImpl → RemotingClient.invoke*
 │   └─ Broker: SendMessageProcessor
 │       ├─ 校验、权限、流控/磁盘水位保护
 │       ├─ MessageStore.putMessage(CommitLog 顺序追加)
 │       │   ├─ AppendMessageCallback 生成 MsgId（含物理偏移）
 │       │   ├─ ReputMessageService 派发至 ConsumeQueue / IndexFile
 │       │   └─ 刷盘（ASYNC_FLUSH / SYNC_FLUSH）
 │       └─ 返回 SEND_OK / 各类错误码（可重试/降级）
 └─ 客户端失败处理（超时/重试次数/切换队列与 Broker）
```



## **1.2 关键实现锚点**

- Producer 侧

  - DefaultMQProducer / DefaultMQProducerImpl#sendKernelImpl
  - MQClientInstance#updateTopicRouteInfoFromNameServer
  - TopicPublishInfo（缓存队列列表/是否可用）
  - MQFaultStrategy、LatencyFaultTolerance（避开“慢/失败”Broker 队列）

  

- NameServer 侧

  - 路由：getTopicRouteInfo（汇总 QueueData + BrokerData）

  

- Broker 侧

  - 请求处理：SendMessageProcessor
  - 存储：DefaultMessageStore#putMessage → CommitLog → ConsumeQueue/IndexFile
  - 刷盘：GroupCommitService（同步）、异步刷盘线程
  - 流控与保护：磁盘水位、PageCache 繁忙、写线程池满





## **1.3 面试易错 & 细节**

- **“Push 发送”不存在**：发送永远是客户端主动发起。
- **路由缓存**：Producer 有本地 TopicPublishInfo，定时与按需刷新；NameServer 都挂了仍可用一段时间（直到缓存过期/路由变化不可见）。
- **默认 Topic（TBW102）**：早期/默认配置下可用来“探路由/自动建 Topic”，生产禁用/收紧该能力更安全。
- **故障规避**：开启延迟容错后，近期失败/高延迟的队列会进入“隔离窗口”，优先发其它队列。
- **MsgId 与 UNIQ_KEY**：返回结果里的 msgId 用于物理定位；MessageClientId/UNIQ_KEY用于追踪（不要拿它做业务幂等主键）。







# **2) 消费路径（Pull/Push/POP → Rebalance → 处理 → 提交位点）**

## **2.1 Push（默认，用长轮询拉取）**

```
Consumer.start (DefaultMQPushConsumer)
 └─ MQClientInstance.start
     ├─ 定时：updateTopicRouteInfoFromNameServer、rebalance
     ├─ 心跳：sendHeartbeatToAllBroker（汇报订阅/能力）
     └─ 拉取服务：PullMessageService
RebalanceImpl（定时/事件触发）
 ├─ 取订阅主题的所有队列（依据 TopicRouteData）
 ├─ 同 Group 在线消费者列表
 ├─ 根据策略分配队列（Averagely / ByCircle / …）
 └─ 新队列建立 ProcessQueue、必要时对顺序队列加锁
拉取循环（每个队列）
 ├─ PullAPIWrapper.pull (long-polling)
 │   └─ Broker: PullMessageProcessor
 │       ├─ 订阅过滤（TagCode/SQL92，Broker 端粗过滤）
 │       ├─ 根据 queueOffset 取消息，可能阻塞至超时
 │       └─ 返回消息批 or “暂时无数据”
 ├─ 消费线程池执行业务回调
 ├─ 成功：提交位点（Cluster：Broker 端；Broadcast：本地）
 └─ 失败：RECONSUME_LATER → 发送回重试主题 %RETRY%Group → 达阈值入 %DLQ%Group
```

要点：

- **Push 本质是客户端拉取**（长轮询，Broker 阻塞等待或返回空），不是服务端真正推送。
- **位点**：Cluster 模式位点存 Broker（ConsumerOffsetManager）；Broadcast 通常本地存。
- **过滤**：Tag 在 Broker 粗过滤，客户端再细判断；SQL92 过滤更重（基于属性），Broker 端执行。
- **顺序消费**：队列级独占 + 单线程串行；需向 Broker 申请队列锁（加/续租），Rebalance/锁失效会打断顺序。



## **2.2 LitePull（手控节奏/批量/背压）**

- 开发者主动 poll() 获取消息；可配置 autoCommit 或手动 commit 指定位点。
- 适合：自控拉取速率、与外部批处理/事务对齐；对“抖动”更敏感，需要自己做背压与线程池治理。





## **2.3 POP（5.x，可见性超时 + ACK，无 queueOffset 提交）**

```
Client.pop(topic, group, invisibilityTimeout)
 └─ Broker: PopMessageProcessor
     ├─ 将 N 条消息标记“不可见（锁定）”
     ├─ 返回消息 + receiptHandle（含过期时间/重放信息）
     └─ Revive 服务定期扫描超时未 ack 的消息 → 重新投递
Client.ack(message, receiptHandle)
 └─ Broker: AckMessageProcessor（确认删除/完成）
失败或超时
 └─ 进入 popRetry / 死信（与 Group 绑定）
```

要点：

- **POP 不维护客户端位点**，靠“不可见时间窗 + ack”完成一次性投递语义。
- 可见性超时过短会导致重复；过长会延迟失败恢复。需要谨慎设定并结合幂等。





## **2.4 Rebalance（谁消费哪些队列）**

- 触发时机：**成员变更**（同组实例上线/下线）、**队列变更**（扩缩容/路由变化）、**订阅变更**、心跳/定时任务。

- 执行流程：

  1. 拉取最新 TopicRouteData + 同组在线实例列表；
  2. 统一排序（实例、队列），套用策略（Averagely / ByCircle / ByMachineRoom…）；
  3. 对于新增分配的队列：创建 ProcessQueue，顺序消费需到 Broker 加“队列锁”；
  4. 对于失去分配的队列：撤销拉取、等待在途处理结束、提交位点/释放锁。

  

- 稳定性建议：提高最小/最大消费线程、限制单批大小、合理 consumeTimeout；顺序消费注意锁续约与超时。





## **2.5 关键实现锚点**

- 客户端

  - DefaultMQPushConsumerImpl / DefaultLitePullConsumerImpl
  - PullMessageService、PullAPIWrapper
  - RebalanceImpl#doRebalance、AllocateMessageQueue* 策略族
  - OffsetStore（Remote/Broker or Local）

  

- Broker

  - PullMessageProcessor、PopMessageProcessor、AckMessageProcessor
  - ConsumerOffsetManager（Cluster 位点）
  - RebalanceLockManager（顺序队列锁）

  



## **2.6 易错点**

- **“消息重复”是常态**（至少一次），必须业务幂等。
- **广播模式**每个实例都全量消费，且位点不在 Broker；不要与 Cluster 混淆。
- Rebalance 抖动可能造成**短暂乱序/重复**；顺序消费要关注锁续期失败与队列迁移。
- POP 场景**没有 queueOffset 提交**，排障时看 receiptHandle / revive 日志。







# **3) 元数据与路由（TopicRouteData、BrokerData）**

## **3.1 NameServer 持有并提供什么**

- 来源：Broker 周期注册/心跳上报（含 Topic→Queue 配置、Broker 角色与地址）。

- 结构（客户端拉取）：

  - **TopicRouteData**

    - List<QueueData>：每个 brokerName 上该 Topic 的队列数、读写权限、同步标志。
    - List<BrokerData>：brokerName → Map<brokerId, brokerAddr>（0 常为 Master，>0 为 Slave；在 DLedger/5.x 语义更偏向 Leader/Followers）。
    - （可选/历史）filterServer 列表。

    

  

- 特性：**NameServer 无状态，不做强一致选主**；客户端可连任意一个拉取。





## **3.2 客户端如何使用路由**

- Producer 端

  - 拉取 TopicRouteData → 构造 TopicPublishInfo（展开为 (topic, brokerName, queueId) 的列表）；
  - 发送时从可用队列里选择（轮询 + 故障规避）。

  

- Consumer 端

  - 拉取 TopicRouteData → 与同组实例列表一起做 Rebalance（按策略将 MessageQueue 分配给实例）。

  

- 缓存与刷新

  - 客户端维持路由缓存，**定时**刷新 + **异常时**主动刷新；
  - NameServer 按**心跳过期**剔除失联 Broker；客户端心跳/拉取失败时会收敛到仍可达的路由。





## **3.3 面试常问点（速答）**

- **TopicRouteData 包含什么？**QueueData（队列与权限）+ BrokerData（brokerName→(brokerId→addr)）。
- **Producer 如何选队列？**基于 TopicPublishInfo 轮询，且可启用延迟容错避开“慢/失败”队列。
- **Consumer 为什么会 Rebalance？**成员/队列/订阅变更或心跳触发；按策略重新分配队列所有权。
- **NameServer 掉了会怎样？**短期不影响已知路由（本地缓存）；路由变化不可见，直到恢复或缓存过期。
- **POP 为什么没有位点？**它依赖“不可见时间窗 + ack receipt”而非 queueOffset。







## **4) 排障口令（发送/消费/路由）**

- 发送失败：看返回码（例如 FLUSH_DISK_TIMEOUT、SEND_MSG_TIMEOUT、磁盘水位拒绝）；排查是否被故障规避、是否被 ACL/权限拒绝。
- 堆积/消费慢：先看 **Group×Topic 的 lag** 与线程池/批量参数；看 Rebalance 是否频繁、单队列是否热点。
- 路由异常：mqadmin topicRoute -n <ns> -t <topic> 对比客户端缓存；检查 Broker 心跳/NameServer 是否一致可见。