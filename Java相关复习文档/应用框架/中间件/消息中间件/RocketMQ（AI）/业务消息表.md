# 方案一

## **1) 当前方案的优点**

- 生产侧：业务提交与“消息记录”同事务，避免“业务成了但消息没发”的丢失窗口（Outbox 核心）。
- 消费侧：**业务成功才标记**已消费，方向正确。
- 定时补偿：考虑了**漏发/迟发**的兜底。
- 有运维告警：超过阈值告警，避免“沉默失败”。







## **2) 主要缺陷与风险点**

1. **固定 10 分钟窗口不自适应**

   积压或慢盘时会“误判未投递”，引入**海量重复补发**，反而加重拥塞。

2. **消费者端用分布式锁去“互斥消费”不必要且脆弱**

   MQ 已保证同组内同队列单实例消费；真正需要的是**幂等**（应对重投/重复），而不是锁。锁有续期/漂移/死锁/可用性问题。

3. **“已消费”状态写到哪？会造成服务耦合**

   若消费者去改 **生产者**的“消息表”，是**跨服务写**，耦合且不可靠。最佳实践是**各写各的**：

   - 生产者维护自己的 **outbox**；
   - 每个消费者维护自己的 **inbox/去重表**（按 *Consumer Group* 维度）。

   

4. **调度器扫描条件与并发控制粗糙**

   仅按“创建时间 > 10 分钟”扫描，会**全表扫描**或走低效索引；多实例并发调度易重复捞同一批。

5. **重试策略过于刚性**

   固定“重试 ≤ 3 次”不区分**临时性** vs **永久性**错误；缺少**指数退避 + 抖动**，容易形成重试风暴。

6. **消息幂等键/关联信息不足**

   只依赖 MQ 自带 MessageId 不够（跨集群不唯一/不可控）。需要**业务幂等键**与**schema 版本**等元数据。







## **3) 升级版方案（在你思路上强化）**

### **3.1 表结构（生产者 Outbox）**

```sql
-- 生产者侧：事务内写入
CREATE TABLE outbox_event (
  id              BIGINT PRIMARY KEY AUTO_INCREMENT,   -- 亦可用 UUID
  biz_key         VARCHAR(128) NOT NULL,               -- 业务幂等键（如订单号）
  event_type      VARCHAR(64)  NOT NULL,               -- 事件类型
  payload         JSON         NOT NULL,               -- 事件数据
  headers         JSON         NULL,                   -- 额外属性（schema_version、traceId 等）
  status          TINYINT      NOT NULL,               -- 0=NEW,1=DISPATCHING,2=SENT,3=DEAD
  attempts        INT          NOT NULL DEFAULT 0,
  next_attempt_at DATETIME     NOT NULL DEFAULT NOW(), -- 下次调度时间（指数退避）
  last_error      VARCHAR(512) NULL,
  mq_msg_id       VARCHAR(128) NULL,                   -- 发送成功后回写
  created_at      DATETIME     NOT NULL DEFAULT NOW(),
  updated_at      DATETIME     NOT NULL DEFAULT NOW(),
  KEY idx_status_time (status, next_attempt_at),
  KEY idx_biz_key (biz_key)
);
```

**关键点**

- “**同事务**”：业务表写入 + outbox_event(…status=NEW) 一起提交。
- 调度器按 status IN (NEW, DISPATCHING) 且 next_attempt_at <= now() 扫描，**FOR UPDATE SKIP LOCKED** 领任务，**并发安全**。
- 发送成功回 SENT；失败回写 last_error，attempts+1，next_attempt_at = now() + backoff(attempts)（指数退避+抖动），若判为永久错误则标 DEAD 并告警。
- **不要**靠“创建时间>10分钟”判断，完全改用 next_attempt_at。

> 发送在**异步调度器**里做，而不是业务请求内同步 send()：

- > 避免“业务长尾阻塞”；

- > 彻底消除“业务提交后进程崩溃未发送”的窗口（调度器会补发）。





### **3.2 表结构（消费者 Inbox/去重表）**

```sql
-- 每个 Consumer Group 自己维护（在消费者自己的库里）
CREATE TABLE inbox_message (
  id            BIGINT PRIMARY KEY AUTO_INCREMENT,
  group_name    VARCHAR(64)  NOT NULL,           -- Consumer Group
  message_key   VARCHAR(128) NOT NULL,           -- 来自 outbox 的幂等键（建议 = outbox.id 或业务 biz_key）
  state         TINYINT      NOT NULL,           -- 0=PROCESSING,1=PROCESSED
  attempts      INT          NOT NULL DEFAULT 0,
  last_error    VARCHAR(512) NULL,
  processed_at  DATETIME     NULL,
  created_at    DATETIME     NOT NULL DEFAULT NOW(),
  updated_at    DATETIME     NOT NULL DEFAULT NOW(),
  UNIQUE KEY uk_group_msg (group_name, message_key)
);
```

**消费流程（Push/LitePull）**（单事务伪代码）

1. BEGIN;

2. INSERT … ON DUPLICATE KEY UPDATE（或先 SELECT … FOR UPDATE）：

   - 若已 PROCESSED → **幂等返回成功**；
   - 若首次/PROCESSING → 拿到**行锁**，继续。

   

3. 执行业务（写 DB/调用下游），**写入与 inbox 同事务**。

4. 成功：UPDATE inbox SET state=PROCESSED, processed_at=now()；COMMIT；返回 CONSUME_SUCCESS。

5. 失败：ROLLBACK；返回 RECONSUME_LATER（或 POP 不 ACK 让其回投）。



> 这样**不需要分布式锁**：唯一约束 + 行级锁即可防并发；重复消息天然幂等。

> message_key 建议使用 **outbox.id** 或**业务幂等键**；一个 Group 一张 inbox（或以 uk_group_msg 区分）。





### **3.3 发送与复制的可靠性（RocketMQ 侧）**

- Broker：flushDiskType=SYNC_FLUSH + **同步复制**（brokerRole=SYNC_MASTER）或 **DLedger 多数派**。
- Producer：同步发送、retryAnotherBrokerWhenNotStoreOK=true，非 SEND_OK 一律当失败重试。
- 消费：**先业务后确认**（Push 返回 CONSUME_SUCCESS / POP ack），失败重试/死信。





### **3.4 调度器（代替“10分钟扫描”）**

- 查询：SELECT … FROM outbox_event WHERE status IN (0,1) AND next_attempt_at <= NOW() ORDER BY next_attempt_at LIMIT N FOR UPDATE SKIP LOCKED;

- 成功：status=SENT，写入 mq_msg_id。

- 失败：分类处理

  - **临时性**（网络、超时、限流）：attempts+1，next_attempt_at = now() + base*2^attempts ± jitter；
  - **永久性**（参数错误、schema 无法解析）：直接 DEAD + 告警（避免重试风暴）。

  

- 并发：调度器可水平扩，多实例**互不抢同一行**（靠 SKIP LOCKED）。

- 速率：根据 MQ TPS/堆积自适应限流。





### **3.5 告警与可观测**

- Outbox：attempts、DEAD 计数、平均 dispatch latency、next_attempt_at 分布。
- Inbox：重复率（命中 uk_group_msg）、平均处理时延、失败率、死信量。
- MQ：Topic×Group 的 **lag**、重试主题量、DLQ 量、Broker 磁盘水位/刷盘/复制时延。
- 阈值告警：用**速率与时延**触发，而非“固定 3 次”。可按事件类型单独阈值与升级路径（工单/钉钉/电话）。









## **4) 你关注的两个点的直接改进**

- **“10 分钟阈值不自适应”**

  → 用 next_attempt_at + 指数退避 + 抖动 + FOR UPDATE SKIP LOCKED，**完全替代固定时间窗口**。

  → 再配合集群的 lag（或发送失败码）动态调整 base，在高压期**自动降速**。

- **“补偿与原消息同时到达会并发消费”**

  → **不需要分布式锁**。Inbox 的唯一键 + 行锁已保证**同一 Group 内同一消息键只会有一个正在处理**；其余重复要么等待锁、要么直接幂等返回。









## **5) 事务边界与清理**

- Outbox 的清理：SENT 留存一段时间（如 7–14 天）后归档/分区删除；DEAD 单独留存。
- Inbox 的清理：PROCESSED 留存（按合规/追溯要求）再归档；或按“最近 N 天 + 按 message_key 分区”。
- **千万不要**让消费者去更新生产者库里的“已消费状态”（跨服务写=高耦合 + 新丢失面）。









## **6) 可选方案对比（你可以二选一或混用）**

- **RocketMQ 事务消息（生产侧）**

  - 优点：将“发送”与“业务本地事务”通过“半消息+回查”弱耦合。
  - 缺点：需要**实现回查接口**、维护回查幂等与超时；跨语言/多库较复杂。
  - 结论：你已采用 Outbox，更通用、简单，**无需再上事务消息**。

  

- **CDC（Debezium）驱动 Outbox**

  - 优点：零轮询、低延迟、写放大更小；
  - 缺点：引入 Kafka/Connect/Debezium 组件与运维复杂度。
  - 结论：量大时可考虑；中小体量用“DB 调度器 + SKIP LOCKED”足够。

  







## **7) 口袋检查清单（落地速配）**

- 生产者：业务提交内 INSERT outbox(status=NEW)；**异步调度器**发 MQ（SYNC_FLUSH+同步复制），失败走指数退避；非 SEND_OK 视失败。
- 消费者：**Inbox 去重**（UNIQUE(group,message_key)）+ **先业务后确认**；不再使用分布式锁。
- 调度器：next_attempt_at + FOR UPDATE SKIP LOCKED 并发拉取；**临时/永久错误分流**；自适应限流。
- 监控：Outbox/Inbox/MQ 三视角指标 + 阈值告警；DEAD 专属看板。
- 清理：分区&归档策略，避免表无界增长。
- 元数据：消息里带 message_key（= outbox.id 或 biz_key）、event_type、schema_version、traceId。





# 方案二

很好的约束：**消息表只在 common 服务**，生产者/消费者都通过 RPC 调 common 完成写/查/改。要做到“可靠、可观测、支持多消费方且不需要分布式锁”，一个“单表”在物理层面不够——因为**一条事件可能被多个 Consumer Group 消费**，必须有**按 Group 维度的消费状态**。所以我给你一套“**中心账本（events）+ 按组消费账本（event_consumption）**”的最小二表模型（逻辑上仍是“一个中心消息账本”），并配套接口与流程。你要是**强行单表**也能做（用 JSON 存 per-group 状态），但查询与并发会变脆——不建议。



下面直接给落地版：表结构 → RPC 接口 → 端到端流程（生产/派发/消费）→ 重试与告警 → 并发控制与分片 → 注意事项与对比。







## **一、数据模型（common 服务内）**

### **1) 事件总账（events）**

```sql
CREATE TABLE events (
  event_id        BIGINT PRIMARY KEY AUTO_INCREMENT,
  publisher_app   VARCHAR(64)  NOT NULL,            -- 生产方标识
  topic           VARCHAR(128) NOT NULL,            -- RocketMQ 目标 Topic（或路由键）
  event_type      VARCHAR(64)  NOT NULL,            -- 业务事件类型
  biz_key         VARCHAR(128) NOT NULL,            -- 业务幂等键（订单号/流水号等）
  payload         JSON         NOT NULL,            -- 事件数据
  headers         JSON         NULL,                -- traceId、schema_version、tags、keys 等
  status          TINYINT      NOT NULL,            -- 0=PREPARED,1=READY,2=DISPATCHING,3=SENT,4=DEAD,5=CANCELED
  attempts        INT          NOT NULL DEFAULT 0,  -- 向 MQ 派发尝试次数
  next_attempt_at DATETIME     NOT NULL DEFAULT NOW(),
  mq_msg_id       VARCHAR(128) NULL,                -- 派发成功后回填
  last_error      VARCHAR(512) NULL,
  created_at      DATETIME     NOT NULL DEFAULT NOW(),
  updated_at      DATETIME     NOT NULL DEFAULT NOW(),
  UNIQUE KEY uk_publisher_biz (publisher_app, event_type, biz_key),
  KEY idx_status_next (status, next_attempt_at),
  KEY idx_topic_status (topic, status, next_attempt_at)
);
```



### **2) 按组消费账本（event_consumption）**

```sql
CREATE TABLE event_consumption (
  id             BIGINT PRIMARY KEY AUTO_INCREMENT,
  event_id       BIGINT       NOT NULL,
  consumer_group VARCHAR(128) NOT NULL,
  state          TINYINT      NOT NULL,            -- 0=PENDING,1=PROCESSING,2=PROCESSED,3=DEAD
  attempts       INT          NOT NULL DEFAULT 0,  -- 消费尝试（监控用）
  next_attempt_at DATETIME    NOT NULL DEFAULT NOW(),
  last_error     VARCHAR(512) NULL,
  processed_at   DATETIME     NULL,
  created_at     DATETIME     NOT NULL DEFAULT NOW(),
  updated_at     DATETIME     NOT NULL DEFAULT NOW(),
  UNIQUE KEY uk_event_group (event_id, consumer_group),
  KEY idx_group_state_next (consumer_group, state, next_attempt_at),
  CONSTRAINT fk_event FOREIGN KEY (event_id) REFERENCES events(event_id)
);
```

> 解释

- > **events**：生产侧唯一事实源（中心消息账本）。

- > **event_consumption**：每个 **Consumer Group** 对同一事件的消费状态（去重、告警、可观测）。

- > 这两个表已经涵盖了你原来“out/in 表”的全部功能，但**集中到 common**；业务侧无需维护本地 inbox/outbox 表。







## **二、RPC 接口（common 服务）**

### **生产端（Producer → common）**

- ReserveEvent(request) → eventId, reservationToken

  入参：publisherApp, topic, eventType, bizKey, payload, headers

  侧写：写 events(status=PREPARED)，返回 eventId 与一次性 reservationToken。

- ConfirmEvent(eventId, reservationToken) → OK

  把 status=READY, next_attempt_at=now()（可幂等）。

- CancelEvent(eventId, reservationToken) → OK

  把 status=CANCELED（可幂等）。

> **关键**：生产业务库里建议**增加一列**存 event_id（或 reservationToken），与业务主事务**同提交**，用于崩溃后补偿确认（下面流程详述）。这满足你“消息表只在 common”——业务库只存“指针”。





### **派发器（common 内部/运维任务）**

- PollEventsToDispatch(limit)：

  取 events WHERE status IN (READY, DISPATCHING) AND next_attempt_at <= NOW() ORDER BY next_attempt_at LIMIT ? FOR UPDATE SKIP LOCKED。

- ReportDispatchResult(eventId, success, mqMsgId?, error?)：

  成功→status=SENT, mq_msg_id=...；失败→attempts++，next_attempt_at=now()+backoff(attempts)±jitter；必要时 DEAD 并告警。

> 说明：**派发到 MQ 的工作由 common 自己做**（内部 job 或独立 worker），不再需要生产者进程里同步发 MQ，从而杜绝“业务提交后，发 MQ 前崩溃”的窗口。





### **消费端（Consumer → common）**

- BeginConsume(eventId, consumerGroup) → {decision}

  逻辑：INSERT … event_consumption(state=PROCESSING) **ON CONFLICT IGNORE**；若已存在：

  - PROCESSED → 返回 ALREADY_DONE（幂等）；
  - PROCESSING → 返回 IN_PROGRESS（可直接幂等继续）；
  - PENDING/DEAD → 切到 PROCESSING 并计数。

  

- FinishConsume(eventId, consumerGroup, success, error?) → OK

  成功→state=PROCESSED, processed_at=now()；失败→attempts++，state=PENDING（或保持 PROCESSING），next_attempt_at=now()+backoff。

- QueryConsumptionState(eventId) / QueryLag(consumerGroup, topic)：可观测。

> **去掉“分布式锁”**：靠 uk_event_group 唯一键 + 行锁即可防重入，且幂等。







## **三、端到端流程（推荐实现）**

### **1) 生产事务（业务库 + common 保证不丢）**

```
业务服务：BEGIN TX
  写业务表（订单等）
  调 common.ReserveEvent(...) → 返回 eventId,reservationToken
  把 eventId 写入业务表一列（与业务同事务提交）
COMMIT
-- 事务提交后（Spring TransactionSynchronization afterCommit）
  调 common.ConfirmEvent(eventId, reservationToken)
  （若失败/宕机）本地补偿任务按业务表里未确认的 eventId 重试 Confirm
```

> 无需 2PC：**只把 eventId“指针”跟业务一起提交**。Confirm 是**事后**动作，可由补偿任务兜底。





### **2) common 派发到 MQ（去掉“10 分钟固定扫描”）**

```
DispatchWorker 定时执行：
  SELECT ... FOR UPDATE SKIP LOCKED 取 READY/DISPATCHING + 到期
  发 RocketMQ（同步发送，开启 retryAnotherBrokerWhenNotStoreOK）
  成功：ReportDispatchResult(…success)
  失败：ReportDispatchResult(…false) → 指数退避 + 抖动 + DEAD 阈值 + 告警
```

> 不再用“创建时间>10分钟”的粗扫描，改为**next_attempt_at 驱动 + 指数退避**，自适应拥塞，避免风暴。





### **3) 消费与去重（无分布式锁）**

```
Consumer 收到 MQ 消息 (包含 eventId/bizKey/…)
  BeginConsume(eventId, group)
    → 若返回 ALREADY_DONE：直接ACK（幂等）
  执行业务（本地事务更新你的业务库，幂等键= bizKey）
  成功：
    FinishConsume(eventId, group, success=true)
    返回 CONSUME_SUCCESS（或 POP ack）
  失败：
    FinishConsume(..., success=false, error=...)
    返回 RECONSUME_LATER（或 POP 不 ack）
```

> “至少一次”由 MQ 兜底；**恰好一次效果**由你的业务幂等键（bizKey 唯一约束/UPSERT）保证。







## **四、重试与告警策略**

- **派发重试（events）**：

  - Backoff：min( base*2^attempts, maxBackoff ) ± jitter；分类**临时/永久**错误（永久错误直接 DEAD+告警）。
  - 指标：派发时延、attempts 直方图、SENT/DEAD 比例、每 Topic TPS。

  

- **消费重试（event_consumption）**：

  - 仅用于**观测与阈值告警**（真正重投由 RocketMQ 完成）。
  - 指标：group×topic lag、PENDING/PROCESSING 数量、attempts、DEAD 数量。
  - 告警：attempts 连续上升或 state=DEAD 超阈值→钉钉/值班。

  







## **五、并发与热点处理**

- **取任务并发**：FOR UPDATE SKIP LOCKED 自然水平扩展，多 worker 不抢同一行。

- **索引**：

  - events(idx_status_next) 支撑派发扫描；
  - event_consumption(idx_group_state_next) 支撑 per-group 看板与卡点定位。

  

- **分区/归档**：

  - 大表建议**按时间范围分区**（月/周）+ TTL 归档；SENT/PROCESSED 老分区冷存或落对象存储。

  

- **SPOF**：common 多副本 + 读写分离；接口全幂等（requestId/reservationToken 双保险）。







# **六、与“强行单表”的对比（如果你非要一张表）**

可以把 event_consumption 挤进 events.consumers JSON（键=group，值=state/attempts/nextAttemptAt…）：

- 优点：名义上一张表。

- 缺点：**并发写冲突**（多个 group 更新同一行）、难以加索引（无法高效查询某 group 的积压/到期）、行膨胀。

  **不推荐**在高并发/多组消费场景使用。







# **七、关键注意事项**

1. **去掉分布式锁**：用唯一键 + 行锁实现幂等与互斥，简单可靠。
2. **生产端不在请求内直接发 MQ**：由 common 派发，避免“提交后崩溃”丢发。
3. **业务库留 eventId 指针**：与业务同事务提交，提供 Confirm 的补偿锚点（无需本地消息表）。
4. **不要再做“未消费 10 分钟就重发 MQ”**：重投由 MQ 负责；common 只负责“**未派发**的重试”。
5. **消息体包含**：eventId, eventType, bizKey, schema_version, traceId, headers…，便于消费幂等与追踪。