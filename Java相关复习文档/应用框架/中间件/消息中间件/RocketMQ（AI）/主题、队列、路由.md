# **一、Topic & Queue 规划**

> 目标：既要顶住吞吐，又要可平滑扩容，还要兼顾（局部）有序与热点治理。

## **1) Queue 数量与吞吐**

- **吞吐来自“并行度 × 单队列能力”**

  - 单队列（同一 MessageQueue）在 Broker 侧读写都是**顺序 append + 顺序读**，吞吐主要受磁盘/页缓存、网络和 CPU 解码限制。
  - **并行度**来自：Topic 的队列数（分布到不同 BrokerName/queueId），以及**消费端线程数 × 被分配的队列数**。

  

- **经验规划（起步→进阶）**

  - 常规业务起步：**每 BrokerName 4–8 个队列**（写读都足够、易于管理）。
  - 高吞吐（数十万 TPS / 大量小消息）：**16–64**（按需增加，但注意下面“扩容与顺序”的代价）。
  - 大消息（≥ 256KB）：优先**优化网络与刷盘策略**，而不是盲目加队列数。

  

- **衡量与上限**

  - 队列太多带来：ConsumeQueue/索引构建开销上升、Rebalance 抖动变大、客户端路由表膨胀。

  - 以**消费侧并发饱和**为基准：

    - 如果每个消费实例最终只分到 1–2 个队列，再加队列意义不大；反之若队列少于实例数会造成**空转**与**不均衡**。

    

  



## **2) 顺序与局部有序**

- **RocketMQ 的“顺序”= 单队列内局部有序**。

  - 想要业务级顺序（同订单、同用户），**生产时用 ShardingKey 固定映射到同一个队列**。
  - 消费端使用“顺序消费”（队列级串行 + 队列锁）。

  

- **扩容影响顺序**

  - **增加队列数会打散既有映射**（除非你在业务层自己维护“key→queueId”的稳定算法/表）。
  - 规避：预留足够队列；或维护“稳定一致性哈希/路由表”，扩容时**只迁移少量 key**。

  



## **3) 热分区（热点队列）**

- **症状**：TPS/积压集中在少数队列；该队列消费者线程饱和、平均延时高。

- **治理手段**

  - **源头治理**：ShardingKey 设计避免偏斜（如 userId 取哈希而非递增 id 段）。
  - **加“分片权重”**：对热点 key 拆更细（key→(queueId, subShard)），消费端做二次聚合。
  - **临时旁路**：为热点业务单独开 Topic；或临时提升该队列的消费并发（更多实例分摊其它队列，释放 CPU 给热点）。
  - **监控**：Queue 维度 TPS/lag/RT 热点可视化是必须项。

  



# **二、路由发现**

> 目标：让客户端稳定拿到“Topic→(brokerName, queueId, perms, addr)”的有效集合，并在故障时快速收敛。



## **1) NameServer 路由拉取**

- 客户端维护 **NameServer 列表**，可连任意一个拉取路由（**无状态**）。

- 拉取得到 TopicRouteData：

  - QueueData：每个 brokerName 上该 Topic 的 **读/写队列数、权限**。
  - BrokerData：brokerName → {brokerId → brokerAddr}（传统主从 0=Master, >0=Slave；DLedger 更关注 Leader/Follower）。

  

- **客户端缓存**：

  - 本地持有 TopicRouteData，**定时刷新 + 按需刷新**（发送失败/不可达会触发主动刷新）。
  - **NameServer 全挂**：短期不影响已知路由（走缓存），但**新变更不可见**。

  



## **2) Broker 注册/注销（对 NS）**

- Broker 周期向 **所有** NameServer 上报：自身 brokerName/brokerId/addr 和包含 Topic 的 TopicConfig。
- NameServer 按**心跳过期**剔除失联 Broker；客户端下次拉路由就会看到精简后的可达集合。



## **3) 路由缓存细节与排障**

- 生产端：TopicPublishInfo（展开后的 MessageQueue 列表 + 可用性）。
- 消费端：Rebalance 前使用最新 TopicRouteData 获取“Topic 的全部队列”。
- 排障命令：mqadmin topicRoute -t <topic>，对比客户端日志中的路由缓存。





# **三、负载均衡（Producer / Consumer）**

> 生产端是“**选队列发**”，消费端是“**选队列拉/收**”。两者都需要在故障时**退避收敛**。

## **1) Producer：轮询队列 + 可达性退避**

- **基本算法**

  - 在 TopicPublishInfo.messageQueueList 中**轮询**选择队列；同一 brokerName 若失败/高延迟会进入**隔离窗口**（延迟容错）。

  

- **延迟容错（故障规避）**

  - 维护“brokerName → 最近失败/RT”的记分板；当某 broker 失败或 RT 超阈值，临时**降低它的优先级/跳过**一段时间。
  - 避免持续打到“半故障/抖动”的节点，提升有效吞吐与成功率。

  

- **发送重试**

  - 同步：retryTimesWhenSendFailed；异步：retryTimesWhenSendAsyncFailed。
  - retryAnotherBrokerWhenNotStoreOK=true：服务端返回“存储非 OK”（刷盘/复制超时等）时，**换 Broker 再试**。

  

- **稳定性建议**

  - 设置合适的 sendMsgTimeout（考虑刷盘与复制时延）；
  - 尽量**同步发送**用于关键链路（语义清晰）；
  - 业务级**批量发送**（单条 ≤ body 限制，批次大小控制在网络 MTU/内核缓冲合适区间）可提升吞吐。

  



## **2) Consumer：Rebalance（触发条件与稳定性）**

- **触发条件**

  - **同组成员变化**（实例上下线、心跳异常）；
  - **队列集合变化**（Topic 扩缩容、Broker 上下线）；
  - **订阅变化**（过滤表达式变了）；
  - 定时/异常触发的**自检**。

  

- **流程要点**

  1. 获取 Topic 全量 MessageQueue 与本组在线实例清单；
  2. 统一排序后应用**分配策略**（见第 4 节）；
  3. 新增队列：创建 ProcessQueue，顺序消费需向 Broker 申请**队列锁**；
  4. 失去队列：停止拉取、等待在途处理落地、提交位点/释放锁。

  

- **稳定性建议**

  - **实例数 ≈ 队列数** 或稍少，有利于分摊均衡；
  - 控制**拉取批量**与**在途消息阈值**（避免某实例压爆）；
  - 顺序消费关注**队列锁续期**与超时；
  - 避免频繁上下线（造成抖动）；
  - POP 模式无位点，用 **ack 超时**与**续租**（changeInvisibleTime）防止“处理中回投”。

  



# **四、Rebalance 策略（AllocateMessageQueue）**

> 影响“**每个消费者实例拿到哪些队列**”。理解它们的分配形态与适用场景。

设：mqCount = |MessageQueue|，cidCount = |consumerIds|，两侧都已排序。

## **1)** Averagely（平均分配，连续切片）

- **规则**：把有序的 MQ 列表按连续区块平均切片给每个消费者。
- **特点**：每个实例拿到**连续的一段队列**；简单稳定，**热点可能集中**在某实例（如果热点队列相邻）。
- **适合**：大多数场景；易于人脑推断/排障。



## **2)** AveragelyByCircle**（环形平均，交错分配）**

- **规则**：按环形轮转分配：消费者 0 拿 mq[0], mq[n], mq[2n]…；消费者 1 拿 mq[1], mq[1+n]…。
- **特点**：队列**交错**给不同实例，**稀释相邻热点**，对热点均衡更友好。
- **代价**：相较连续切片，实例间**跨 BrokerName 的概率更高**（对顺序消费不影响，但网络路径更分散）。



## **3)** ByMachineRoom**（按机房/逻辑分区偏置）**

- **规则**：根据 brokerName/标签的“机房”归属，只把**同机房**的队列分给该机房内的消费者。
- **特点**：**本地性优化**（跨 AZ/IDC 流量减少）；容灾/多活常用。
- **前提**：消费者也要注明/可识别机房属性；跨房故障时要允许“降级跨房分配”。



## **4)** **ByConfig**（按配置白名单）

- **规则**：人为配置“实例 → 队列集合”的映射。
- **特点**：**强约束/固定绑定**，一般用于**旁路运维**、压测、迁移割接。
- **风险**：动态性差、配置漂移易错；不建议常态使用。

> 小贴士

- > 对**热点分布未知**或“相邻热点较多”，优先 AveragelyByCircle。

- > 需要**本地性/机房隔离**，用 ByMachineRoom。

- > 需要严格可控的“**只让 A 实例消费这些队列**”，临时使用 ByConfig。





## **源码/类名锚点（便于深挖）**

- **路由**：TopicRouteData / QueueData / BrokerData，MQClientInstance#updateTopicRouteInfoFromNameServer
- **生产选队列**：DefaultMQProducerImpl#sendKernelImpl，MQFaultStrategy / LatencyFaultTolerance
- **消费拉取**：PullMessageService / PullAPIWrapper
- **Rebalance**：RebalanceImpl#doRebalance，AllocateMessageQueueAveragely / AveragelyByCircle / ByMachineRoom / ByConfig
- **顺序队列锁**：RebalanceLockManager





## **实战 Checklist（可直接照着做）**

- **队列规划**：按“目标并发 × 单队列能力”定初值（4–8 起步，热点/高并发到 16–64），优先**源头避免键偏斜**。
- **顺序需求**：ShardingKey→固定队列；避免随意加队列；必要时用一致性哈希/路由表做“只迁少量 key”的扩容。
- **路由**：多 NS，客户端定时+按需刷新；生产失败时主动刷新；mqadmin topicRoute 常备。
- **生产负载**：开启**延迟容错**与“非 OK 换 Broker 重试”，合理 sendMsgTimeout；关键链路**同步发送**。
- **消费负载**：实例数与队列数相匹配；控制批量与在途阈值；顺序消费关注队列锁续租。
- **策略选择**：默认 Averagely；热点多→AveragelyByCircle；跨机房→ByMachineRoom；割接/压测→ByConfig（临时）。
- **监控**：队列维度 TPS/lag/RT 热点图；Rebalance 频率；失败码/退避命中率；NameServer 可达性。



# **Producer 指定队列（业务分片键取模） vs Broker 故障隔离**

- **默认行为**

  - 如果你不指定队列，Producer 会在 TopicPublishInfo.messageQueueList 上**轮询**选择，并结合**延迟容错（LatencyFaultTolerance）**避开近期失败/高延迟的 broker。
  - 如果你**显式指定了队列**（比如根据业务 id 取模到 queue2），RocketMQ **会优先遵循你的选择**，不会再走默认的“轮询+容错”逻辑。

  

- **后果**

  - 一旦你固定投到 queue2（而 queue2 所在 broker 此时抖动或在隔离窗口），Producer 仍然会尝试往该 broker 发送。
  - 如果最终返回失败（例如 SEND_OK 以外状态），你需要在应用侧处理：要么重试，要么降级，要么记录异常。
  - RocketMQ 内置的“隔离窗口”只对**自动选队列**的情况生效，对**显式指定**的队列不干预。

  

- **常见做法**

  - **顺序消息场景**（必须 key→固定队列）：不能依赖故障规避，必须自己兜底，例如：

    - 检测队列不可用时，写入**重试/补偿表**，由后台任务定时重发；
    - 或者在业务层做**“丢队列转账”策略**，比如“顺序性没那么强”的场景允许切换到临时备用队列。

    

  - **普通消息场景**（不要求严格顺序）：尽量不要显式绑定，交给 MQ 的容错轮询更稳。

  